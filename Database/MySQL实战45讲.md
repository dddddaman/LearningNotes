## 01 | 基础架构：一条SQL查询语句是如何执行的？

MySQL的基本架构示意图。

SQL语句在MySQL各个功能模块中的执行过程：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190831154353799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA2NTcwOTQ=,size_16,color_FFFFFF,t_70)

大体来说，MySQL 可以分为 Server 层和存储引擎层两部分

- Server层包括：连接器、查询缓存、分析器、优化器、执行器等。
- 存储引擎负责数据的存储和提取。常用的存储引擎InnoDB、MyISAM

#### 连接器

使用长连接有时候MySQL占用内存涨得特别快，因为MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。长期连接可能导致内存占用太大，被系统强行杀掉（OOM），也就是MySQL异常重启了。

解决方案：

* 定期断开长连接。
* MySQL 5.7后的版本，可以通过执行 mysql_reset_connection 来重新初始化连接资源。该过程不需要重连和重新做权限验证。

#### 查询缓存

执行查询后，执行结果会被存入查询缓存中，如果查询命中缓存，MySQL不需要执行后面的操作，可以直接返回结果。效率高。

但是，查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。建议大多数情况下不要使用查询缓存。MySQL8.0版本把查询缓存功能已去除。

#### 分析器

```mysql
select * from T where ID = 10;
```

MySQL需要从命令里识别字符串分别是什么，代表什么。

1. 词法分析：识别 “select” 关键字，识别表名 “T” ，识别列ID “ID” 
2. 语法分析：判断该 SQL 语句是否满足 MySQL 语法

#### 优化器

当表里有多个索引，或者一个语句有多表关联（join）的时候，不同方案执行效率不同，优化器决定选择使用哪个方案。

#### 执行器

校验权限后，执行器根据表的引擎定义，去使用这个引擎提供的接口。


## 02| 日志系统：一条SQL更新语句是如何执行的？

与查询流程不一样的是，更新流程还涉及两个重要的日志模块：redo log（重做日志）和binlog（归档日志）。

### 日志模块：redo log

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190831154516315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA2NTcwOTQ=,size_16,color_FFFFFF,t_70)

在做更新操作时，如果每一次更新操作都立即写进磁盘，那么整个过程IO成本、查找成本都很高。redo log就是为了解决这一问题，MySQL通过WAL（Write-Ahead Logging）技术（先写日志，再写磁盘），先把更新操作写入redo log日志，并更新内存，这时更新操作就完成了，同时InnoDB引擎会在适当的时候（系统比较空闲时），将一批操作更新到磁盘里面。如果redo log写满时，就不能再执行新的更新，得停下来先擦掉redo log一些记录，写入磁盘，再执行新的更新。

有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe


### 日志模块：binlog

redo log和binlog不同点：

1. redo log 是 InnoDB 引擎特有的；binlog（归档日志）是MySQL的Server层实现的，所有引擎都可以使用
2. redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1”。binlog两种模式：statement格式记录SQL语句，row格式记录行的内容，两条（更新前和更新后）。
3. redo log 是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

update语句执行流程：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190831154617226.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA2NTcwOTQ=,size_16,color_FFFFFF,t_70)

图中浅色框表示再InnoDB内部执行的，深色框表示在执行器中执行的。

将redo log的写入拆成了两个步骤：prepare 和 commit，这就是“两阶段提交”。

“两阶段提交”的目的是为了让两份日志之间的逻辑一致。保证一致性。


数据恢复过程：

- 首先，找到最近的一次全量备份，从这个备份恢复到临时库；
- 然后，从备份的时间点开始，将备份的binlog依次取出来，重放到中午误删表之前的那个时刻。
- 最后把表数据从临时库取出来，按需要恢复到线上库去。

redo log用于保证crash-safe能力。

innodb_flush_log_at_trx_commit这个参数设置成1，表示每次事物的redo log都直接持久化到磁盘（建议设置为1）。

sync_binlog这个参数设置为1，表示每次事物的binlog都持久化到磁盘（建议设置为1）。


## 03 | 事务隔离：为什么你改了我还看不见？

事物就是要保证一组数据库操作要么全部成功，要么全部失败。

事物支持是在引擎层实现的，但并不是所有引擎都支持事物，MyISAM引擎就不支持事物。

多事务同时执行的时候，可能会出现的问题：脏读、不可重复读、幻读。

SQL标准的事物隔离级别：

- 读未提交。是指一个事物还没提交时，它做的变更就能够被其他事物看到。
- 读提交。是指一个事物提交之后，它做的变更才会被其他事物看到。
- 可重复读。是指一个事物在执行过程中看到的数据，总是跟这个事物启动时看到的数据是一致的。
- 串行化。串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突时，后访问的事物必须等待前一个事物执行完成，才能继续执行。


MySQL事物启动方式：

- 显式启动事务语句， begin 或 start transaction。提交语句commit，回滚语句rollback。
- set autocommit=0，这个命令会将这个线程的自动提交关闭。


## 04 | 深入浅出索引（上）

索引的出现是为了提高数据库查询效率，就像书的目录一样。

常见的索引模型：

- 哈希表
- 有序数组
- 搜索树

哈希表这种结构适用于只有等值查询的场景，比如 Memcached及其他一些nosql引擎。由于哈希表存储不是有序的，因此做区间查询的速度是很慢的。

而有序数组在等值查询和范围查询场景中的性能就都非常优秀。有序数组只适用于静态存储引擎，由于它是有序存储，因此在插入数据时，要挪动后面的所有数据，成本太高。

为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。因此使用N叉数，而不是二叉树。


### InnoDB 的索引模型

在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。InnoDB使用的是B+树索引模型。

每一个索引在 InnoDB 里面对应一棵 B+ 树。B+树能够很好地配合磁盘的读写特性，减少单词查询时访问磁盘的次数。

主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引。

非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引。

普通索引的查询方式是先搜索普通索引树，找到主键值，再去搜索主键索引树。这个过程也被称为回表。也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应尽量使用主键查询。

数据页的分裂与合并：当数据页已满时，有新的数据插入就会申请新的数据页，并把部分数据移动过去，这个过程称为数据页的分裂，不仅性能会受影响，而且数据页的利用率也会降低。当相邻的两个数据页由于数据删除，导致利用率很低时，就会将数据页合并。

从性能和存储空间方面考量，自增主键往往是更合理的选择。



## 05 | 深入浅出索引（下）

有普通索引查询主键值，再回到主键索引树搜索的过程称为回表。那如何优化索引避免回表呢：

**覆盖索引**。指的是普通索引树上，节点已经包含了要查询的信息，也就是普通索引“覆盖了”我们的查询需求，因此就不用再使用主键查询，避免了回表。由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的优化手段。

**最左前缀原则**。B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符。

联合索引如何安排字段顺序：

- 如果可以通过改变字段顺序，从而少维护一个索引，那么将是优先考虑的
- 第二个考虑原则是空间

### 索引下推

而 MySQL 5.6 引入的索引下推优化，在索引遍历过程中，对索引包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。


## 06 | 全局锁和表锁 ：给表加个字段怎么有这么多阻碍？

数据库锁设计的初衷是处理并发问题。根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。

### 全局锁

全局锁就是对整个数据库实例进行加锁。要想使数据库处于只读状态，加全局读锁，使用命令：Flush table with read lock（FTWRL）。使用全局读锁后，其他线程的数据更新语句、数据定义语句和更新类事物的提交语句都将被阻塞。

全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都select出来存成文本。

MyISAM不支持可重复读的事物隔离级别，因数据库备份时要使用FTWRL（只读全局锁）来进行备份，期间只能进行读操作。因此建议使用InnoDB引擎。InnoDB使用mysqldump备份时，建议使用参数--single-transaction，导数据前会启动一个事务，确保拿到一致性视图

设置全库只读：Flush table with read lock（FTWRL）、   set global readonly=true。建议使用FTWRL，原因是：

- 有些系统中readonly值会用来做逻辑判断，比如判断是主库还是从库。因此，修改global变量的方式影响面更大，不建议使用。
- 异常处理机制有差异。执行FTWRL命令后，如果由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到正常更新状态。而设置readonly后，如果客户端发生异常，MySQL会一直保持readonly状态，数据库长时间处于不可写状态，风险较高。

InnoDB：使用官方自带的逻辑备份工具mysqldump的single transaction参数

MyISAM：使用 FTWRL 或者 set global readonly=true


### 表级锁

表锁一般是引擎不支持行锁时才被用到。

MySQL表级锁有两种：表锁、元数据锁（meta data lock， MDL）

表锁语法：lock tables ... read/write。  unlock tables 释放锁。客户端断开的时候自动释放。

MDL: 不需要显示使用，在访问一个表时会被自动加上。MDL的作用是，保证读写的正确性。当对一个表做增删改查、结构变更操作的时候，加MDL锁。

- 读锁之间不互斥，多个线程可以同时对一张表增删改查
- 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，两个线程要同时给一个表加字段，其中一个要等另一个执行完才能执行。
- 事务中的MDL锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。
- 如果一个表有频繁的查询语句，而且客户端有重试机制，这时改变表结构可能会导致库的线程爆满，从而整个库挂掉。当有多个查询在执行时，语句结束后并不会马上释放MDL读锁，而是等到整个事物提交后释放，这时更改表结构会被阻塞，它被阻塞后，之后的所有操作都会被阻塞，整个表就不可读写了。

如果应用程序里有lock tables这样的语句，可能的情况：

* 系统还在用MyISAM这类不支持事务的引擎，需要尽快升级引擎
* 引擎升级了，但是代码没升级，需要把lock tables和unlock tables换成begin和commit


### 如何安全的给小表加字段：

- 解决长事物。如果要做DDL变更的表有长事物在执行，要考虑暂停DDL，或者kill掉这个长事物。如果这个表是热点表，有频繁的请求，那么kill未必管用，因为新的请求马上就来了。可以通过在alter table语句里面设定等待时间，如果等待时间内能拿到MDL写锁就执行，拿不到就放弃，不用阻塞后面的业务语句，之后再重试这个过程。
- 注意：MySQL中的DDL代表着数据库定义语句，用来创建数据库中的表、索引、视图、存储过程、触发器等。常用的语句关键字有：CREATE, ALTER, DROP, TRUNCATE, COMMENT, RENAME。
- MariaDB 已经合并了AliSQL的这个功能，支持DDL NOWAIT / WAIT N 这个语法

```mysql
ALTER TABLE tbl_name NOWAIT add column ...
ALTER TABLE tbl_name WAIT N add column ...
```

### 问题

备份一般都会在备库上执行，你在用-single-transaction方法做逻辑备份的过程中，如果主库上的一个小表做了一个DDL，比如给表加了一列，这时候，从备库上会看到什么现象呢？

答：

假设这个 DDL 是针对表 t1 的， 这里我把备份过程中几个关键的语句列出来：

```mysql
Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;
Q2:START TRANSACTION  WITH CONSISTENT SNAPSHOT；
/* other tables */
Q3:SAVEPOINT sp;
/* 时刻 1 */
Q4:show create table `t1`;
/* 时刻 2 */
Q5:SELECT * FROM `t1`;
/* 时刻 3 */
Q6:ROLLBACK TO SAVEPOINT sp;
/* 时刻 4 */
/* other tables */
```

备份流程：

1. 在备份开始的时候，为了确保 RR（可重复读）隔离级别，再设置一次 RR 隔离级别 (Q1);
2. 启动事务，这里用 WITH CONSISTENT SNAPSHOT 确保这个语句执行完就可以得到一个一致性视图（Q2)；
3. 设置一个保存点，这个很重要（Q3）；
4. show create 是为了拿到表结构 (Q4)，
5. 然后正式导数据 （Q5），
6. 回滚到 SAVEPOINT sp，在这里的作用是释放 t1 的 MDL 锁 （Q6）。当然这部分属于“超纲”，上文正文里面都没提到。

DDL 从主库传过来的时间按照效果不同，我打了四个时刻。题目设定为小表，我们假定到达后，如果开始执行，则很快能够执行完成。

题目答案：

1. 如果在 Q4 语句执行之前到达，现象：没有影响，备份拿到的是 DDL 后的表结构。
2. 如果在“时刻 2”到达，则表结构被改过，Q5 执行的时候，报 Table definition has changed, please retry transaction，现象：mysqldump 终止；
3. 如果在“时刻 2”和“时刻 3”之间到达，mysqldump 占着 t1 的 MDL 读锁，binlog 被阻塞，现象：主从延迟，直到 Q6 执行完成。
4. 从“时刻 4”开始，mysqldump 释放了 MDL 读锁，现象：没有影响，备份拿到的是 DDL 前的表结构。



## 07| 行锁功过：怎么减少行锁对性能的影响？

* 行锁是针对数据库表中行记录的锁。
* 存储引擎InnoDB支持行锁，MyISAM不支持行锁。
* 两阶段锁协议：在InnoDB事物中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。

如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。原因是这个锁的时间最少，减少了事务之间的锁等待，提升了并发度。

举例：

比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更新。事务 B 的 update 语句会被阻塞，直到事务 A 执行 commit 之后，事务 B 才能继续执行。事务 A 持有的两个记录的行锁，都是在 commit 的时候才释放的。

[7.1图片]

假设你负责实现一个电影票在线交易业务，顾客 A 要在影院 B 购买电影票。交易操作包括：

1. 从顾客 A 账户余额中扣除电影票价；
2. 给影院 B 的账户余额增加这张电影票价；
3. 记录一条交易日志。

试想如果同时有另外一个顾客 C 要在影院 B 买票，那么这两个事务冲突的部分就是语句 2 了。因为它们要更新同一个影院账户的余额，需要修改同一行数据。

根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你把语句 2 安排在最后，比如按照 3、1、2 这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。


### 死锁和死锁检测

死锁：当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态

应对死锁有两种策略：

- 一种策略是，直接进入等待，直到超时。超时时间通过参数innodb_lock_wait_timeout来设置。默认为50s。
- 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事物得以继续执行。开启这个逻辑，通过设置参数innodb_deadlock_detect 为on来实现。默认为on。


如果主动死锁检测过多，就会消耗大量的CPU资源，这是一个时间复杂度是 O(n) 的操作。比如1000个并发线程同时更新一行记录，那么死锁检测就是100万量级（热点行更新导致的性能问题）。

如何解决：

- 如果能够保证这个业务不会出现死锁，可以临时把死锁检测关掉。不推荐。出现死锁业务发生回滚，这时业务是无损的；关掉死锁检测意味着可能出现大量的超时，这时业务有损的。
- 另一种是控制并发度。比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。客户端无法控制，要在服务端控制。在中间件实现或修改MySQL源码，基本思路是对于相同行的更新，在进入引擎之前排队。这样在 InnoDB 内部就不会有大量的死锁检测工作了。
- 把一行变为逻辑上的多行，减少锁等待个数。这需要业务上的支持和设计：影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。

### 问题

如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到：

- 第一种，直接执行 delete from T limit 10000;
- 第二种，在一个连接中循环执行 20 次 delete from T limit 500;
- 第三种，在 20 个连接中同时执行 delete from T limit 500。

你会选择哪一种方法呢？为什么呢？

答：第二种方式是相对较好的。第一种方式（即：直接执行 delete from T limit 10000）里面，单个语句占用时间长，锁的时间也比较长；而且大事务还会导致主从延迟。第三种方式（即：在 20 个连接中同时执行 delete from T limit 500），会人为造成锁冲突。


## 08 | 事务到底是隔离的还是不隔离的？

begin/start transaction 命令并不是事物的的起点，在执行的它们之后的第一个操作InnoDB表的语句，事物才真正启动。要想立即启动事物可以使用start transaction with consistent snapshot这个命令。

**MySQL中的两种视图：**

- 一个是view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果
- 一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view。用于支持RC（Read Committed读提交）和RR（Repeatable Read可重复读）隔离级别的实现

在可重复读隔离级别下,事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。
InnoDB 利用了“所有数据都有多个版本”的这个特性,实现了“秒级创建快照”的能力。

在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。而数据版本的可见性规则，就是基于数据的 row trx_id 和这个一致性视图的对比结果得到的。

[图8.2]

对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种情况：

1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；

2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；

3. 如果落在黄色部分，那就包括两种情况

   a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；

   b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。

即，一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：：

1. 版本未提交,不可见
2. 版本已提交，但是在创建视图数组之后提交的，不可见
3. 在视图数组创建之前提交的，可见

更新规则：更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”(current read)。
除了 update 语句外，select 语句如果加锁，也是当前读。

可重复读的核心就是一致性读(consistent read)；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。

读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是:

- 在可重复读隔离级别下,只需要在事务开始的时候创建一致性视图,之后事务里的其他查询都共用这个一致性视图；
- 在读提交隔离级别下,每一个语句执行前都会重新算出一个新的视图。

InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。

* 对于可重复读，查询只承认在事务启动前就已经提交完成的数据；
* 对于读提交，查询只承认在语句启动前就已经提交完成的数据；
* 而当前读，总是读取已经提交完成的最新版本。

为什么表结构不支持“可重复读”？这是因为表结构没有对应的行数据，也没有 row trx_id，因此只能遵循当前读的逻辑。

### 问题

我用下面的表结构和初始化语句作为试验环境，事务隔离级别是可重复读。现在，我要把所有“字段 c 和 id 值相等的行”的 c 值清零，但是却发现了一个“诡异”的、改不掉的情况。请你构造出这种情况，并说明其原理。

```mysql
mysql> CREATE TABLE `t` (
  `id` int(11) NOT NULL,
  `c` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB;
insert into t(id, c) values(1,1),(2,2),(3,3),(4,4);
```

复现出来以后，请你再思考一下，在实际的业务开发中有没有可能碰到这种情况？你的应用代码会不会掉进这个“坑”里，你又是怎么解决的呢？

[图9.]

这样，session A 看到的就是我截图的效果了。还有另外一种场景：

[图9.]

这个操作序列跑出来，session A 看的内容也是能够复现我截图的效果的。这个 session B’启动的事务比 A 要早，其实是上期我们描述事务版本的可见性规则时留的彩蛋，因为规则里还有一个“活跃事务的判断”，我是准备留到这里再补充的。当我试图在这里讲述完整规则的时候，发现第 8 篇文章《事务到底是隔离的还是不隔离的？》中的解释引入了太多的概念，以致于分析起来非常复杂。因此，我重写了第 8 篇，这样我们人工去判断可见性的时候，才会更方便。【看到这里，我建议你能够再重新打开第 8 篇文章并认真学习一次。如果学习的过程中，有任何问题，也欢迎你给我留言】用新的方式来分析 session B’的更新为什么对 session A 不可见就是：在 session A 视图数组创建的瞬间，session B’是活跃的，属于“版本未提交，不可见”这种情况。



## 09 | 普通索引和唯一索引，应该怎么选择？

### 查询过程

* 对于普通索引来说，查找到满足条件的第一个记录 (5,500) 后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录。

* 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。

那么，这个不同带来的性能差距会有多少呢？答案是，微乎其微。

你知道的，InnoDB 的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB。

因为引擎是按页读写的，所以说，当找到 k=5 的记录的时候，它所在的数据页就都在内存里了。那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。

当然，如果 k=5 这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些。但是，我们之前计算过，对于整型字段，一个数据页可以放近千个 key，因此出现这种情况的概率会很低。所以，我们计算平均性能差异时，仍可以认为这个操作成本对于现在的 CPU 来说可以忽略不计。

### 更新过程

#### change buffer

当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存中，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。

将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。

显然，如果能够将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。

唯一索引要确定其唯一性，因此必须要将数据页读取到内存中，判断其是否唯一。所以唯一索引用不到change buffer，普通索引可以用到。

change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。

change buffer 使用的是buffer pool里的内存，change buffer实际上是可以持久化的数据，将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。系统后台会定期merge，数据库正常关闭也会merge。

#### 举例

我们还是用第 4 篇文章《深入浅出索引（上）》中的例子来说明，假设字段 k 上的值都不重复。如果要在这张表中插入一个新记录 (4,400) 的话，InnoDB 的处理流程为：

**第一种情况是，这个记录要更新的目标页在内存中。这时，InnoDB 的处理流程如下：**

* 对于唯一索引来说，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束；
* 对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束。

这样看来，普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的 CPU 时间。

**第二种情况是，这个记录要更新的目标页不在内存中。这时，InnoDB 的处理流程如下：**

* 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；
* 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。

将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。

### change buffer 的使用场景

对于读多写少的业务来说，页面在写完之后马上读的概率很小，因此很多更新操作会缓存之change buffer 中，这样的话使用效果会很好。但是业务是写入之后会马上读取的话，会触发merge，这样随机访问IO的次数不会减少，同时又增加了change buffer的维护成本，这样反而起到了副作用。

### 索引选择和实践

普通索引和唯一索引应该怎么选择，其实，这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。所以，我建议你尽量选择普通索引。

如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能。

### change buffer 和 redo log

redo log 主要节省的是随机写磁盘的 IO 消耗(转成顺序写)，而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。

redo log 与 change buffer(含磁盘持久化) 这2个机制，不同之处在于，优化了整个变更流程的不同阶段。 先不考虑redo log、change buffer机制，简化抽象一个变更(insert、update、delete)流程：

1. 从磁盘读取待变更的行所在的数据页，读取至内存页中。（涉及 随机 读磁盘IO） 
2. 对内存页中的行，执行变更操作
3. 将变更后的数据页，写入至磁盘中。（涉及 随机 写磁盘IO）

Change buffer机制，优化了步骤1，避免了随机读磁盘IO 

Redo log机制，优化了步骤3，避免了随机写磁盘IO，将随机写磁盘，优化为了顺序写磁盘(写redo log，确保crash-safe) 

在我们mysql innodb中，change buffer机制不是一直会被应用到，仅当待操作的数据页当前不在内存中，需要先读磁盘加载数据页时，change buffer才有用武之地。 redo log机制，为了保证crash-safe，一直都会用到。有无用到change buffer机制，对于redo log这步的区别在于，用到了change buffer机制时，在redo log中记录的本次变更，是记录new change buffer item相关的信息，而不是直接的记录物理页的变更。

### 问题

1、评论区大家对“是否使用唯一索引”有比较多的讨论，主要是纠结在“业务可能无法确保”的情况，作如下解释：

首先，业务正确性优先。咱们这篇文章的前提是“业务代码已经保证不会写入重复数据”的情况下，讨论性能问题。如果业务不能保证，或者业务就是要求数据库来做约束，那么没得选，必须创建唯一索引。这种情况下，本篇文章的意义在于，如果碰上了大量插入数据慢、内存命中率低的时候，可以给你多提供一个排查思路。

然后，在一些“归档库”的场景，你是可以考虑使用普通索引的。比如，线上数据只需要保留半年，然后历史数据保存在归档库。这时候，归档数据已经是确保没有唯一键冲突了。要提高归档效率，可以考虑把表里面的唯一索引改成普通索引。

2、change buffer 一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致 change buffer 丢失呢？change buffer 丢失可不是小事儿，再从磁盘读入数据可就没有了 merge 过程，就等于是数据丢失了。会不会出现这种情况呢？

答：不会丢失

虽然是只更新内存，但是在事务提交的时候，我们把 change buffer 的操作也记录到 redo log 里了，所以崩溃恢复的时候，change buffer 也能找回来。

那么，merge 的过程是否会把数据直接写回磁盘？

merge的流程：

1. 从磁盘读入数据页到内存（老版本的数据页）；
2. 从 change buffer 里找出这个数据页的 change buffer 记录 (可能有多个），依次应用，得到新版数据页；
3. 写 redo log。这个 redo log 包含了数据的变更和 change buffer 的变更。

到这里 merge 过程就结束了。这时候，数据页和内存中 change buffer 对应的磁盘位置都还没有修改，属于脏页，之后各自刷回自己的物理数据，就是另外一个过程了。

## 10 | MySQL为什么有时候会选错索引？

```mysql
//创建一个表
CREATE TABLE `t` ( 
    `id` int(11) NOT NULL, 
    `a` int(11) DEFAULT NULL, 
    `b` int(11) DEFAULT NULL, 
    PRIMARY KEY (`id`), 
    KEY `a` (`a`), 
    KEY `b` (`b`)
) ENGINE=InnoDB；

//我们往表 t 中插入 10 万行记录，取值按整数递增，即：(1,1,1)，(2,2,2)，(3,3,3) 直到 (100000,100000,100000)
delimiter ;;
create procedure idata()
begin
  declare i int;
  set i=1;
  while(i<=100000)do
    insert into t values(i, i, i);
    set i=i+1;
  end while;
end;;
delimiter ;
call idata();

//查询数据
mysql> select * from t where a between 10000 and 20000;
```

使用哪个索引是由 MySQL 来确定的，确切的说是优化器的工作。


### 优化器的逻辑

优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。

优化器选择索引的影响因素：

* 查询扫描行数。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。
* 是否使用临时表
* 是否排序

扫描行数的判断方法：

MySQL 根据索引的“区分度”来估算记录数。一个索引上不同的值越多，这个索引的区分度就越好。

* 基数（cardinality）：索引上不同值的个数。这个基数越大，索引的区分度越好。
* 命令 “show index from T ”来查询表 T 中一个索引的基数

MySQL 采样统计方法（获取索引的基数）：

采样统计时，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。

在 MySQL 中，有两种存储索引统计的方式，可以通过设置参数 innodb_stats_persistent 的值来选择：

* 设置为 on 的时候，表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10。
* 设置为 off 的时候，表示统计信息只存储在内存中。这时，默认的 N 是 8，M 是 16。

命令 ”analyze table t“，可以用来重新统计索引信息。在实践中，如果发现命令 “explain” 的结果预估的rows值跟实际情况差距比较大，可以使用这个方法处理。


### 索引选择异常和处理

大多数时候优化器都能找到正确的索引，但偶尔会选错索引。比如：原本可以执行得很快的 SQL 语句，执行速度却比你预期的慢很多，这时有以下处理方法：

1. 采用force index强行选择一个索引。缺点：变更的及时性。线上系统修改SQL语句还要测试、发布等，不够敏捷。
2. 修改语句，引导MySQL使用我们期望的索引。例如：在逻辑结果一致时可以把：order by b limit 1 改为 order by b,a limit 1（注意文章的上下文）。缺点：根据数据特征诱导了一下优化器，不具备通用性。
3. 在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。缺点是找到更合适的索引比较难。如果误用的索引没必要存在，可以删除。

```mysql
set long_query_time=0;
select * from t where a between 10000 and 20000; /*Q1*/
select * from t force index(a) where a between 10000 and 20000;/*Q2*/
```

### 总结

* explain：模拟优化器执行SQL查询语句，从而知道MySQL是如何处理你的SQL语句的，分析你的查询语句或是表结构的性能瓶颈
* analyze table：解决索引统计信息不准确导致的问题
* force index：解决优化器误判的情况

### 问题

前面我们在构造第一个例子的过程中，通过 session A 的配合，让 session B 删除数据后又重新插入了一遍数据，然后就发现 explain 结果中，rows 字段从 10001 变成 37000 多。而如果没有 session A 的配合，只是单独执行 delete from t 、call idata()、explain 这三句话，会看到 rows 字段其实还是 10000 左右。这是什么原因呢？

答：

delete 语句删掉了所有的数据，然后再通过 call idata() 插入了 10 万行数据，看上去是覆盖了原来的 10 万行。

但是，session A 开启了事务并没有提交，所以之前插入的 10 万行数据是不能删除的。这样，之前的数据每一行数据都有两个版本，旧版本是 delete 之前的数据，新版本是标记为 deleted 的数据。

这样，索引 a 上的数据其实就有两份。

然后你会说，不对啊，主键上的数据也不能删，那没有使用 force index 的语句，使用 explain 命令看到的扫描行数为什么还是 100000 左右？（潜台词，如果这个也翻倍，也许优化器还会认为选字段 a 作为索引更合适）

是的，不过这个是主键，主键是直接按照表的行数来估计的。而表的行数，优化器直接用的是 show table status 的值。这个值的计算方法，我会在后面有文章为你详细讲解。


## 11 | 怎么给字符串字段加索引？

### 前缀索引

MySQL支持前缀索引，你可以定义字符串的一部分作为索引。默认地，创建时不指定长度索引就会包含整个字符串。

比如，这两个在 email 字段上创建索引的语句：

```mysql
//index1 索引里面，包含了每个记录的整个字符串
mysql> alter table SUser add index index1(email); 
//index2 索引里面，对于每个记录都是只取前 6 个字节
mysql> alter table SUser add index index2(email(6));
```

这两种不同的定义在数据结构和存储上的区别：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190831153158870.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA2NTcwOTQ=,size_16,color_FFFFFF,t_70)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190831153242687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA2NTcwOTQ=,size_16,color_FFFFFF,t_70)



```mysql
select id,name,email from SUser where email='zhangssxyz@xxx.com';
```

分析上面语句，在两个索引的定义下是如何执行的：

* 如果使用的是 index1（即 email 整个字符串的索引结构），执行顺序是这样的：
  1. 从 index1 索引树找到满足索引值是’zhangssxyz@xxx.com’的这条记录，取得 ID2 的值；
  2. 到主键上查到主键值是 ID2 的行，判断 email 的值是正确的，将这行记录加入结果集；
  3. 取 index1 索引树上刚刚查到的位置的下一条记录，发现已经不满足 email='zhangssxyz@xxx.com’的条件了，循环结束。
* 如果使用的是 index2（即 email(6) 索引结构），执行顺序是这样的：
  1. 从 index2 索引树找到满足索引值是’zhangs’的记录，找到的第一个是 ID1；
  2. 到主键上查到主键值是 ID1 的行，判断出 email 的值不是’zhangssxyz@xxx.com’，这行记录丢弃；
  3. 取 index2 上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出 ID2，再到 ID 索引上取整行然后判断，这次值对了，将这行记录加入结果集；
  4. 重复上一步，直到在 idxe2 上取到的值不是’zhangs’时，循环结束。

可以发现，使用前缀索引后，可能会导致查询语句读数据的次数变多。

因此，使用前缀索引有以下特点：

* 优点：前缀索引只取字符串前几位，比整个字符串索引占用空间更小。
* 缺点：可能会增加额外的记录扫描次数。因为依据前缀查询后，要去主键索引查找判断是否正确，这时有可能前缀一样后面的字符串不一致，就需要再去字符串索引查找，这就增加了记录扫描次数（回主键查找次数）。
* 最优方法：使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。

如何确定前缀的长度：

1、统计出这个列上有多少个不同的值：

```mysql
mysql> select count(distinct email) as L from SUser;
```

2、依次取不同长度的前缀来统计不同值，看哪个值不小于 L * 95%（5%接受区分度损失比例）：

```mysql
mysql> select
  count(distinct left(email,4)）as L4,
  count(distinct left(email,5)）as L5,
  count(distinct left(email,6)）as L6,
  count(distinct left(email,7)）as L7,
from SUser;
```


### 前缀索引对覆盖索引的影响

如果使用 index1（即 email 整个字符串的索引结构）的话，可以利用覆盖索引，从 index1 查到结果后直接就返回了，不需要回到 ID 索引再去查一次。而如果使用 index2（即 email(6) 索引结构）的话，就不得不回到 ID 索引再去判断 email 字段的值。

使用前缀索引就用不上覆盖索引对查询性能的优化了。例如在不使用前缀索引情况下，覆盖索引含有要查询的信息，就不用回表查询ID索引了。使用了前缀索引，还需查询ID索引确认是否是要查找的记录。


### 其他方式

索引选取的越长，占用的磁盘空间就越大，相同的数据页能放下的索引值就越少，搜索的效率也就会越低。

如果遇到前缀的区分度不够好的情况时，该怎么办：

1. 第一种方式是**使用倒序存储**。有时字符串倒叙会有很好的区分度。

   ```mysql
   mysql> select field_list from t where id_card = reverse('input_id_card_string');
   ```

2. 第二种方式是**使用hash字段**。你可以在表上再创建一个整数字段，来保存身份证的校验码，同时在这个字段上创建索引。

   ```mysql
   mysql> alter table t add id_card_crc int unsigned, add index(id_card_crc);
   mysql> select field_list from t where id_card_crc=crc32('input_id_card_string') and id_card='input_id_card_string'
   ```


### 异同点

相同点：都不支持范围查询。倒叙存储是按照倒叙字符串排序的，hash字段只能支持等值查询。

区别：

1. 从占用额外空间来看，倒叙存储方式在主键索引上，不会消耗额外的存储空间，而hash需要增加一个字段。当然倒叙存储方式使用4个字节的前缀长度应该是不够的，再长一点，消耗和hash也差不多。
2. 在CPU消耗方面，倒叙需要调用reverse函数，hash需要调用crc32()函数。reverse函数的额外消耗CPU资源会更小些。
3. 从查询效率上看，使用hash字段方式的查询性能相对更稳定一些。hash每次查询平均扫描次数接近1。倒叙使用前缀索引，会增加回表查询次数。


### 总结：

1. 直接创建完整索引,这样可能比较占用空间;
2. 创建前缀索引,节省空间,但会增加查询扫描次数,并且不能使用覆盖索引;
3. 倒序存储,再创建前缀索引,用于绕过字符串本身前缀的区分度不够的问题；
4. 创建 hash 字段索引,查询性能稳定,有额外的存储和计算消耗,跟第三种方式一样,都不支持范围扫描。

### 问题

如果你在维护一个学校的学生信息数据库，学生登录名的统一格式是”学号 @gmail.com", 而学号的规则是：十五位的数字，其中前三位是所在城市编号、第四到第六位是学校编号、第七位到第十位是入学年份、最后五位是顺序编号。

系统登录的时候都需要学生输入登录名和密码，验证正确后才能继续使用系统。就只考虑登录验证这个行为的话，你会怎么设计这个登录名的索引呢？

答：

由于这个学号的规则，无论是正向还是反向的前缀索引，重复度都比较高。因为维护的只是一个学校的，因此前面 6 位（其中，前三位是所在城市编号、第四到第六位是学校编号）其实是固定的，邮箱后缀都是 @gamil.com，因此可以只存入学年份加顺序编号，它们的长度是 9 位。

而其实在此基础上，可以用数字类型来存这 9 位数字。比如 201100001，这样只需要占 4 个字节。其实这个就是一种 hash，只是它用了最简单的转换规则：字符串转数字的规则，而刚好我们设定的这个背景，可以保证这个转换后结果的唯一性。


## 12 | 为什么我的MySQL会“抖”一下？

### 场景现象

一条 SQL 语句，正常执行的时候特别快，但是有时也不知道怎么回事，它就会变得特别慢，并且这样的场景很难复现，它不只随机，而且持续时间还很短。看上去，这就像是数据库“抖”了一下。

### 你的 SQL 语句为什么变“慢”了

* 脏页：当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”
* 干净页：内存数据写入到磁盘后，内存和磁盘上的数据数据页的内容一致，称为“干净页”

不论是脏页还是干净页，都在内存中。把内存里的数据写入磁盘的过程，术语就是 flush。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190831153904760.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA2NTcwOTQ=,size_16,color_FFFFFF,t_70)


因此不难想象，平时执行很快的更新操作，其实就是在写内存和日志，而MySQL偶尔“抖”一下的那个瞬间，可能就是在刷脏页。

**什么情况下回触发数据库的flush过程：**

1. 第一种情况是，InnoDB的redo log写满了（粉板满了）。这时系统会停止所有更新操作，把checkpoint往前推进，为redo log留出空间继续写。checkpoint推进的这段范围上的脏页都flush到磁盘上后，redo log才可继续写。

2. 第二种情况是，系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是脏页，就要先将脏页写到磁盘。

   刷脏页一定会写盘，保证了每个数据页有两种状态：

   - 一种是内存里存在，内存里就肯定是正确的结果，直接返回；
   - 另一种是内存里没有数据，就可以肯定数据文件上是正确的结果，读入内存后返回。这样的效率最高。

3. 第三种情况是，MySQL认为系统“空闲”的时候。redo log很快会记完，所以要见缝插针的找时间刷“脏页”。

4. 第四种情况是，MySQL正常关闭的情况。这时，MySQL会把内存的脏页都flush到磁盘上。这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。

**上述四种场景对性能的影响：**

1. 第三种是空闲时候写，第四种是数据库关闭时候写，通常不会关注性能问题。
2. 第一种是redo log写满了，要flush脏页。这时系统不再接受更新了，所有的更新都会堵住。这种情况是 InnoDB 要尽量避免的。
3. 第二种是内存不够了，现将脏页写到磁盘，数据页淘汰机制（最久不使用），这种情况其实是常态。InnoDB 用缓冲池(buffer pool)管理内存，缓冲池中的内存页有三种状态：
   1. 还没使用的
   2. 使用了且是干净页
   3. 使用了且是脏页

InnoDB 的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。

而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久不使用的数据页从内存中淘汰掉。如果要淘汰的是一个干净页，就直接释放出来复用；如果淘汰的是脏页，要先flush到磁盘，变成干净页后，才能复用。

刷脏页虽然是常态，但是出现以下这两种情况，会明显影响性能：

1. 一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；
2. 日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的。

所以，InnoDB 需要有控制脏页比例的机制，来尽量避免上面的这两种情况。

### InnoDB 刷脏页的控制策略

首先告诉InnoDB所在主机的IO能力，这样InnoDB才能知道需要全力刷脏页的时候，可以刷多快。通过 innodb_io_capacity 这个参数设置，该值建议设置成磁盘的IOPS（可以通过fio工具测试）。

InnoDB怎么控制引擎按照“全力”的百分比来刷脏页？

刷盘速度要参考两个因素：

* 一个是脏页比例
* 一个是redo log写盘速度

与此相关的两个定义：

* F1(M)：参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认值是 75%，InnoDB 会根据当前的脏页比例（假设为 M），算出一个范围在 0 到 100 之间的数字，记为 F1(M)
* F2(N)：InnoDB 每次写入的日志都有一个序号，当前写入的序号跟 checkpoint 对应的序号之间的差值，我们假设为 N。InnoDB 会根据这个 N 算出一个范围在 0 到 100 之间的数字，这个计算公式可以记为 F2(N)。 N 越大，算出来的值越大

根据算得的 F1(M) 和 F2(N) 两个值，取其中较大的值记为 R，之后引擎就可以按照 innodb_io_capacity 定义的能力乘以 R% 来控制刷脏页的速度。



![在这里插入图片描述](https://img-blog.csdnimg.cn/20190831154138620.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA2NTcwOTQ=,size_16,color_FFFFFF,t_70)

无论是你的查询语句在需要内存的时候可能要求淘汰一个脏页，还是由于刷脏页的逻辑会占用 IO 资源并可能影响到了你的更新语句，都可能是造成你从业务端感知到 MySQL“抖”了一下的原因。

要避免这种情况，就要合理地设置 innodb_io_capacity 的值，并且平时要多关注脏页的比例，不要让它经常接近75%。

其中，脏页比例是通过 Innodb_buffer_pool_pages_dirty/Innodb_buffer_pool_pages_total 得到的：

```mysql
select VARIABLE_VALUE into @a from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_dirty';
select VARIABLE_VALUE into @b from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_total';
select @a/@b;
```

而 MySQL 中的一个机制，可能让你的查询会更慢：在刷脏页时，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷（该行为在机械硬盘时代很有意义，可以减少很多随机 IO。相同的逻辑操作减少随机 IO 就意味着系统性能的大幅度提升。但在SSD时IOPS往往就不是瓶颈了）。

innodb_flush_neighbors参数来控制这个行为：

* 值为1时，会有上述的“连坐”机制；
* 值为0时，表示不找邻居，只刷自己。MySQL8.0，该参数默认值为0。

### 总结

1. 利用 WAL 技术，数据库将随机写转换成了顺序写，大大提升了数据库的性能。这个机制后续需要的刷脏页操作和执行时机。
2. 由此也带来了内存脏页的问题。脏页会被后台线程自动 flush，也会由于数据页淘汰而触发 flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些。
3. 因此介绍了控制刷脏页的方法和对应的监控方式。

### 问题

一个内存配置为 128GB、innodb_io_capacity 设置为 20000 的大规格实例，正常会建议你将 redo log 设置成 4 个 1GB 的文件。但如果你在配置的时候不慎将 redo log 设置成了 1 个 100M 的文件，会发生什么情况呢？又为什么会出现这样的情况呢？

答：

每次事务提交都要写 redo log，如果设置太小，很快就会被写满，也就是下面这个图的状态，这个“环”将很快被写满，write pos 一直追着 CP（checkpoint）。这时候系统不得不停止所有更新，去推进 checkpoint。这时，你看到的现象就是磁盘压力很小，但是数据库出现间歇性的性能下跌。


## 13 | 为什么表数据删掉一半，表文件大小不变？

### 现象

我的数据库占用空间太大，我把一个最大的表删掉了一半的数据，怎么表文件的大小还是没变？

### 主要内容

数据库表的空间回收。为什么简单地删除表数据到不到表空间回收的效果，如何正确回收空间？

InnoDB表包含两部分：表结构定义和表数据。MySQL8.0之前，表结构是存在 .frm 为后缀的文件里，8.0版本则允许把表结构定义放在系统数据表中了。

首先说明为什么简单地删除表数据达不到表空间回收的效果，然后再介绍正确回收空间的方法。
